# Related Links 

[Chip Huyen CS 329S course notes](https://stanford-cs329s.github.io/syllabus.html)
[Chip Huyen ML Interviews Book](https://huyenchip.com/ml-interviews-book/)
[FAANG ML system design interview guide](https://www.reddit.com/r/learnmachinelearning/comments/1glkkve/faang_ml_system_design_interview_guide/)
[Interview Framework](https://www.tryexponent.com/blog/machine-learning-system-design-interview-guide)
[System Design in ML - Geeks for Geeks](https://www.geeksforgeeks.org/system-design/system-design-in-machine-learning/)
[Educative.io - Grokking System Design Interview](https://www.educative.io/courses/grokking-the-system-design-interview) 

# To-do list 
[[To-do - Fall 2025 Studying]]


# Research vs Production

![[Pasted image 20250919171611.png]]


* Oftentimes, research teams prioritize **throughput** (to achieve faster training) while production teams prioritize **latency** (to prioritize faster predictions and better UX). 
* Latency is critical for real-time use cases such as autocomplete, fraud detection, TikTok recommendations
* Throughput is more important for training, daily recommendation generation, large-scale data analytics
* Trade-off between latency and throughput: 
  * If we process each request individually, latency is minimized 
  * If we batch, then we increase throughput but some requests must wait longer 
  * Can find optimize the tradeoff via dynamic batching, request prioritization, or having different models with different latency/throughput profiles
* Interpretability and fairness are often more important in industry than academia since they might be needed for ethical, practical, or legal reasons. 


# Requirements for ML Systems

* **Reliability**: the system should continue to perform the correct function at the desired level of performance even in the face of adversity
  * ML Systems often fail silently
* **Scalability:** the system should keep up with growth without massively increasing required resources
  * **Autoscaling** is a key concept here
* **Maintainability:** it's important to structure a project such that different contributors with different backgrounds are comfortable working on it 
* **Adaptability:** system should allow updates without service interruption and evolve as needed

# Objectives
* **Decouple objectives:** If there are multiple objectives (orthogonal, conflicting, etc.) it's a good idea to decouple them first - this makes model development and maintenance easier. E.g., rather than predict for post quality and engagement with one model, build separate model for each and combine the outputs into a weighted score. 

# Data Sources
* **User-generated data:** tends to be mal-formatted (requires lots of checking and validation) and require fast processing (low latency). 
* **System-generated data:** generated by logs, events, model predictions, etc. Useful for anomaly detection. Often can be batch- rather than real-time- processed. 
		* Problem specific to logs: the storage needs tend to explode quickly. We can use low-frequency access storage instead, e.g. S3 Glacier. 
* **Internal databases:** data generated by services and enterprise applications in the company. 
* **Third-party data:** data collected by companies on people who are not their customers (e.g., Health Gorilla)

# Data Formats and Storage
* **Data serialization:** the process of converting data into a format that can be stored or transmitted. ![[Pasted image 20250919211255.png]]
* JSON: language-independent, human-readable, flexible
	* Text file
* CSV: row-major. Consecutive elements in a row are stored next to each other
	* Allows faster data writes (good for transactions)
	* Text file
* Parquet: column-major. Consecutive elements in a column are stored next to each other
	* Allows faster column reads (good for analytics)
	* Binary file: more compact 
	* De facto standard for Hive table storage

## NumPy vs Pandas
* Pandas is column-major: it's basically a dictionary of arrays
* NumPy can be *either* row-major or column-major

# Data Models
* **Relational model**: data is structured into tables, where each row is a tuple where order does not matter. ![[Pasted image 20250919213034.png]]
	* One downside is that due to data normalization, we typically require a lot of expensive joining to get all the necessary data attributes. 
* **NoSQL:** non-relational, we don't require a schema for the data. It is a movement rather than a specific model
	* **Document model:** one document is a single string with a unique key that is used to retrieve it. We prioritize the *content* of each item.
		* Each document could have a different schema. E.g., you could have an array of JSON's with each JSON being a different "document" each with its own schema. 
		* One upside of the document model has better locality than the relational model: rather than querying multiple tables, everything necessary lives in one document.
		* However, running filters and queries is much slower: e.g. to find all books with price < $25, you need to read all documents, extract prices, compare them to 25, and then return the documents. 
		* Many modern database systems support both relational and document 
	* **Graph model:** we prioritize the *relationships* between items using nodes and edges. 

# Structured vs Unstructured Data
* **Structured data** follows a predefined data model, or a **data schema**. 
	* Pro: this makes data easy to analyzet
	* Con: you have to commit to a predefined schema, so if you want to make changes then you need to retrospectively update all the data. 
* **Unstructured data** is more flexible, allowing us to bend with changing input formats or business requirements. 
	* Note that the data still needs structure *at some point* to be useful. You are essentially pushing the need to "structure" the data to the reader rather than the writer of the data. 
* **Data warehouses** store structured data. 
* **Data lakes** store unstructured data .They're useful for storing raw data before processing. ![[Pasted image 20250919214438.png]]

# ETL or ELT? 
* With the advent of big data, it became popular to ask "why don't we just load all the data and then figure out what to do with it later"? That way you don't have to figure out the schema up front. 
* However, this makes storage expensive and search very slow. Moreover, cloud applications require standardized data. 

# Training Data

## Sampling
* Necessary when you don't have access to all data, it's infeasible to process all the data, or want to speed up training/evaluation 
* **Non-probability sampling:** typically riddled with selection biases 
	* Convenience sampling
	* Snowball sampling: future samples are selected based on existing samples. E.g. to scrape legitimate Twitter accounts, you start with a small number of accounts and recursively scrape their followers. 
	* Judgment sampling:experts decide what samples to include
	* Quota sampling: select samples for slices of data without randomization, e.g. 100 responses from 30-40 year olds, 100 from 40-50 year olds, etc. 
* **Probability sampling:**
	* Simple random sampling
		* Pro: simple
		* Con: rare classes or feature values might not appear 
	* Stratified sampling
		* Pro: all classes are represented
		* Con: not always possible
	* Weighted sampling
		* Pro: lets you upweight data from more valuable subsets or account for biases in the sampling 
		* Closely related to sample weighting 
	* Importance sampling
		* Used when you can't directly sample from P(x) but you can sample from Q(x); we sample from Q(x) and weight by $P(x)/Q(x)$ 
	* Reservoir sampling
		* Useful for continually incoming data ![[Pasted image 20250920111035.png]]
## Labeling
* Hand Labels
	* Cons:
		* Expensive
		* Potential threat to data privacy
		* Slow
	* Leads to slow iteration speed and makes your model less adaptive
* Label Multiplicity
	* What if there are labels from different sources? What if annotators/moderators disagree with each other?
	* The higher the level of expertise needed, the higher the potential for disagreement
	* To minimize this, we need a very clear problem definition which is incorporated into annotator training
* Data Lineage
	* When data comes from different sources, it's important to keep track of the origin of each sample and its label
	* This helps us flag potential biases and identify areas of improvement
* What if there are no or insufficient hand labels available?
	* **Weak supervision:** use a collection of weak heuristics to generate labels. E.g., Snorkel.
	* **Semi-supervision:** from a small set of initial labels, we extrapolate to other labels. There are many possible approaches to this
		* Self-training: start by training a model against existing labels, then make predictions on unlabeled samples. Take only the predictions with very high or low probability scores, add those to the training set, and repeat. 
		* Label propagation: assume that samples that are close in feature space have the same labels. We can use KNN or clustering. 
		* Perturbations: we can generate new training samples by perturbing initial samples but keeping the same labels. 
	* **Transfer Learning:** apply a model trained on one task (presumably with many samples) to the task at hand (the *downstream task*) as a starting point. 
		* E.g., we can use a base language model as a starting point but then apply it to sentiment analysis or question answering. 
		* Zero-shot transfer learning: we apply the pretrained base model directly
		* Fine-tuning: we make small changes to the base model, e.g., retrain the last layer of a neural network or train the whole model for a few epochs. 
	* **Active Learning:** we improve the efficiency of data labels by actively choosing which samples to train against. ![[Pasted image 20250920114028.png]]
		* For a binary classifier, we might choose to label and then train only against samples where the model is very uncertain
		* Query-by-committee: train the sample model with different sets of hyperparameters, and then select samples with the most disagreement among the committee.  
		* We can even synthesize samples to learn against in the region of input space with the most uncertainty
	* **Class Imbalance** 
		* Can also occur in continuous labels with high skew, e.g., healthcare costs. Median is low but 95th percentile is extremely high. 
		* ML typically does not work well in imbalanced cases for a few reasons
			* *Insufficient signal* on minority classes
			* Easy to get stuck in *non-optimal solution* where model learns simple heuristic 
			* *Asymmetric costs of error* on the rare class 
		* Choose the right evaluation metric
			* AUROC can be misleading since it is possible to have a high AUROC even when performance on the minority class is poor 
			* Prefer F1-score or AUPRC
		* Resampling: oversampling or undersampling
			* SMOTE is a popular oversampling method, but only works with low-dimensional data
			* We can also use two-phase learning: train model against resampled but finetune against original
		* Algorithm-level methods
			* Class-balanced loss function: upweight the loss on minority classes
		  
# Feature Engineering

## Data Augmentation

* Simple Transformations
	* NLP: synonyms
	* Vision: rotations
* Perturbations:
	* Improve generalizability of models, especially neural networks
* Synthesized data

## Missing Values
* Types of missingness:
	* **Missing not at Random:** reasons why a value is missing is because of the value itself. E.g. people with higher incomes tend not to self-report their income. 
	* **Missing at Random:** reasons why a value is missing is due to another variable. E.g., males with higher income tend not to self-report their income. 
	* **Missing completely at Random:** there is no pattern to the missingness. This is very rare. 
* Methods to deal with missing values
	* **Deletion:** either column deletion or row deletion. Simple but deletion can destroy valuable information or create biases in your model. E.g., removing all rows with missing incomes can remove all rows with gender=male.
	* **Imputation:** 
		* Mean/mode imputation for continuous data
		* Default value, e.g., empty string or -1 for categorical/ordinal data
		* KNN imputation

## Scaling
**Feature scaling:** if you don't scale features, models will place greater weight on features with larger values. 
	* **Min-max scaling:** $x' = \frac{x - \min(x)}{\max(x) - \min(x)}$ 
	* **Normal standardization:** scale the variable to have $N(0,1)$ normal distribution. $x' = \frac{x - \bar{x}}{\sigma}$ 
	* **Log transformation:** transform a skewed (log-normal) variable into normal
	* **Be careful of data leakage!!**  

## Discretization
We can discretize continuous variables into categorical variables, e.g., income, age. This can help deal with extreme outliers which might otherwise skew our predictions. 

## Categorical Features
* How to handle features where the categories are dynamic, e.g., they are constantly changing in production? 
	* **Hashing trick:** we use a hash space with a very large number of possible values, e.g. 2^18
		* There is a possibility of collision, but it should be very rare 
* Feature Crossing
	* We combine two or more features to generate new features
	* This lets us capture non-linear relationships between features, e.g. `marital_status x number_of_children` might be more predictive than either one by itself for buying a house. 
	* Very helpful for linear models. 
	* Can lead to overfitting because it can lead to many more features. 

# Model Development

## Model Selection
1. Avoid the state-of-the-art (SOTA) trap: just because a model does better in an academic setting, doesn't mean it is fast or cheap or interpretable enough for an industry application
2. Start with the simplest models
	* Note that "simple" might mean a complex model that is pretrained, e.g., BERT
3. Avoid human biases in model selection - e.g., don't run 100 hyperparameters for one model and 5 for another
4. Evaluate good performance now vs good performance later using learning curves. It gives you a sense of whether model performance is likely to improve with more training data in the future. E.g., a neural network model might outperform GBDT in the future with more data. 
5. Evaluate trade-offs: interpretability vs performance, compute cost vs performance
6. Understand your model's assumptions, e.g., IID, normal distribution, decision boundaries, conditional independence

## Ensembles
- Bagging: bootstrap aggregating, reduces variance. Sample with replacement to create different datasets and train model on each dataset. Helps with models that tend to overfit, e.g., decision trees. 
	- Used in random forests
- Boosting: train a sequence of weak learners iteratively, each one trained to account for the misses of the previous ones. 
	- Used XGBoost and LightGBM
- Stacking: train base learners from the training data and then train a meta-learner to combine their outputs 

## Experiment Tracking and Versioning
- Important to track model artifacts:
	- Hyperparameters
	- Loss curves on train and valid
	- Model performance metrics, e.g., F1, precision, recall
	- Speed, e.g. # steps/second, # tokens/second
	- System performance, GPU/CPU utilization, memory usage
- Versioning 
	- Code versioning: git
	- Data versioning: DVC

## Model Debugging
For neural networks, this is a simple recipe to debug poor performance:
1. Start simple and gradually add more components: simplify the architecture
2. Overfit a single small batch (10 samples) to make sure your model can get 100% accuracy
3. Set a random seed to give consistency

## Phases of ML Development
1. Start with a heuristic: *"If you think that machine learning will give you a 100% boost, then a heuristic will get you 50% of the way there.”*
2. Simplest ML models
3. Optimize simple models
4. Move to more complex models

## Distributed Training
* Useful if your training dataset doesn't fit in memory
* You will need algorithms for preprocessing, shuffling, batching out-of-memory
* **Data Parallelism:** split data on multiple machines, train on all of them, and accumulate gradients. Each machine has its own copy of the model.
	* However, this leads to a "straggler problem" where some machines are slower to finish gradient descent than others and since it's Synchronous SGD, you end up waiting for the slower machines. 
	* You can use Asynchronous SGD: this can lead to gradient staleness and slower convergence but generally not a problem 
* **Model Parallelism:** different components of the model are trained on different machines. E.g., different machines handle different layers of the network. 
	* Can run into the same problem as above where you end up waiting.
* **Pipeline Parallelism:** ![[Pasted image 20250921115302.png]]


# Offline Evaluation

* Baseline: compare your model to a reasonable baseline, e.g.:
	* Continuous: average or median
	* Categorical: mode or random
	* Simple heuristic
	* Human performance
* It's important to make sure your evaluation data reflects production data as closely as possible
	* Perturbation: For audio, NLP, images, we can add perturbation to the eval data
	* Invariance tests: certain changes to input, e.g., race or gender, shouldn't change outcomes
	* Directional Expectation tests: increasing lot size should increase house price
	* Model calibration: measure calibration, use Platt scaling if necessary
	* Slice-based evaluation: evaluate F1 etc. on various subsets of the data. Simpson's paradox. You can find critical slides from either heuristics or error analysis. 

# Model Deployment

- Deploying a model is easy; deploying it *well* (high uptime, low latency, high throughput, monitoring, logs, alerts, telemetry) is hard
- **Warning:** Separating deployment modes into clean categories (batch vs online, edge vs cloud) is a convenient but leaky abstraction

## Batch vs Online Prediction

- **Online Prediction:** predictions are generated as soon as a request is received. Also known as **synchronous prediction** or **on-demand prediction**. 
	- Traditionally done via REST APIs 
- **Batch Prediction:** generated periodically (daily, monthly) or on trigger. Also known as **asynchronous prediction** or **offline prediction**. 
	- E.g., Netflix might generate user recommendations every 4 hours. 
- In batch prediction, only batch features are used. In online prediction, we can use both streaming and batch features. E.g., we can use both the immediate patient request data (symptoms) and patient history. 
- It is possible to have a hybrid system
	- To reduce average latency, you can generate precomputed predictions for popular queries and stream predictions for less popular queries. 
	- You can precompute predictions for all users and serve them as requests arrive. 

* Batch prediction is largely a product of legacy systems, e.g., MapReduce, Spark. But when companies want to make online inference, they need different systems. This can lead to bugs: ![[Pasted image 20250921222947.png]]
	* Frameworks like Apache Flink are meant to unify this. 
* What do you do if you want to do real-time predictions but your model inference is too slow? There are 3 choices:
	1. Inference optimization
	2. Faster hardware
	3. Inference optimization

## Model Compression
- **Low-rank factorization:** convert high-dimensional tensors into low-dimensional tensors. Very popular with neural networks. 
- **Knowledge distillation:** a small model is trained to mimic a larger model. Not very common since training the teacher model can be very expensive. 
- **Pruning:** in the context of neural networks, reduce the number of non-zero parameters but keep the same architecture. 
- **Quantization:** the most common compression method. Use fewer bits to represent the parameters. Instead of 32 bits or 64 bits, we use 16 bits per float number. 
	- This speeds up both training and inference while reducing memory footprint. 
	- Note that we can also quantize post-training. 

## Cloud vs Edge
* Cloud is convenient but expensive
* Edge is cheaper and you can run apps more flexibly, e.g., with no signal, and don't need to worry about network latency. Also can keep sensitive user data on -device. 


# Data Distribution Shifts and Monitoring

## Natural Labels and Feedback Loop
* Problems with natural labels (e.g., ETA on Google Maps) generate easy evaluations. 
	* We can also set up user feedback to generate such labels. 
* Short feedback loops: rec systems with quick actions, e.g., TikTok. 
* Long feedback loops: fraud detection on credit card transactions, users might not see fraudulent charge until end of month. 
	* These can cause long periods of time to lapse before model problems are noticed. 

# Causes of ML System Failures
* There are two types of metrics that we care about for ML systems: **operational metrics** and **ML performance metrics**
	* Operational metrics might be latency, uptime
	* ML performance metrics might be accuracy, precision
* **Software system failures** are failures that would have occurred even to a non-ML system
	* Dependency failure: e.g., package or codebase breaks
	* Deployment failure: e.g.,accidentally deploy wrong binary or system has wrong permissions
	* Hardware failure: e.g., CPU overheats
	* Downtime or crashing: server crashes
* **ML failures** are specific only to ML systems:
	* Data collection and processing problems
	* Poor hyperparameters
	* Difference between training and production data
	* Data distribution shifts
	* Edge cases
	* Degenerate feedback loop
* Production data differing from training data
	* Critical implicit assumption in ML systems: training and production data should come from the same, stationary distribution 
	* This assumption is often violated in practice, e.g. because of selection or sample bias during training, data distribution shifts, or different feature engineering in training vs production. This is known as the **train-serving skew**. 
* **Edge cases**
	* Edge cases are closely related to outliers, but in this context, edge case := an example where a model performs significantly worse than other examples. Edge cases and outliers are not necessarily 1-to-1. 
* **Degenerate Feedback Loop**
	* When predictions influence feedback, which influences subsequent label generation and training. Very common with recommender systems. 
	* E.g., a rec system might rank song A marginally higher than song B but because A gets more clicks over time, the system keeps ranking A higher and higher. This is known as "exposure bias", "popularity bias", "filter bubble". ![[Pasted image 20250922130512.png]]
	* It is possible to detect degenerate feedback loops by measuring the *diversity* of the system's recommendations - how often does the system recommend the same top N songs versus the long tail of less popular sons?

## Data Distribution Shifts
- Source distribution: distribution the model is trained against
- Target distribution: distribution the model runs inference against 
- Let's say our model is trying to learn $P(Y|X)$ 
	- **Covariate shift:** $P(X)$ changes but $P(Y|X)$ is the same
	- **Label shift:** $P(Y)$ changes but $P(X|Y)$ is the same
	- **Concept drift:** $P(Y|X)$ changes but $P(X)$ is the same
- Covariate shift
	- Your breast cancer detection model is trained primarily on women > age 40, but in production it's applied to women < age 40 as well. 
	- Difficult to account for this in training, but if you know what the target distribution looks like then you can do sample-weighting to make your train data look more like your production data.
- Label shift
	- Imagine there is a drug that every woman now takes which reduces the risk of breast cancer. This reduces $P(Y)$ but doesn't change $P(X|Y)$ 
	- Covariate shift can lead to label shfit
- Concept drift
	- Input distribution remains the same but conditional distribution of output changes
- There are other types of changes
	- Feature change: new features are added, older features removed, set of possible values for feature changes
	- Label schema change: set of possible values for Y changes 
- Detecting data distribution shifts
	- If ground truth is available on a short feedback loop, we can monitor accuracy metrics
	- If not, we can monitor distribution $P(X)$ 
	- You can use 2-sample hypothesis tests to test for difference between source and target distributions
		- Might require time-series tests
- Addressing data distribution shifts
	- If the training dataset is large enough, your model should learn such a general distribution that it can handle shifts in production. Popular in academia. 
	- The most popular approach in industry is to do frequent retraining, either from scratch or fine-tuning. 
	- You can also choose features that are likely to be more stable: ![[Pasted image 20250922145626.png]]


## Monitoring

- **Monitoring** vs **Observability**: Monitoring is the act of tracking, measuring, and logging various metrics that can help us determine when something goes wrong. Observability is setting up our system in a way that gives us visibility into the system to help us investigate what went wrong (i.e. your system isn't a black box).
	- Observability is also called "instrumentation". Examples include: adding timers to functions, counting nulls, tracking input transformations, logging unusual events.
	- Observability is part of monitoring: without some observability, monitoring is impossible. 
- **Operational metrics** are the most important thing to start monitoring. These are related to the health of your ML system. These have 3 parts:
	- Network the ML system runs on
	- Machine the ML system runs on
	- Application that the ML system runs
	- Examples include latency, throughput, number of requests, % that return 2XX code, CPU/GPU utilization, memory utilization
	- **Availability**: how often the system is available to users. This is measured by **uptime**. Conditions to define "up" are set in the SLO or SLA, e.g., "median latency < 200 ms and 99p latency < 2s"
- **ML metrics** generate artifacts which fall into 4 buckets: raw inputs, features, predictions, and accuracy. ![[Pasted image 20250922150502.png]]
	- Accuracy-related metrics: user feedback, natural labels, secondary metrics. 
	- Predictions: easy to monitor shifts in prediction distribution with 2-sample hypothesis tests. Changes in predictions usually map to changes in inputs. 
	- Features: feature validation. min, max, median, values belong to predefined set, etc. This is often less useful than it might seem.
		1. Companies might have hundreds or thousands of features across models
		2. Feature changes might not => perf degradation, leading to alert fatigue
		3. Feature extraction typically done across multiple services, e.g., Spark => NumPy. 
		4. Feature schemas can change over time, leading to false alarms. 
- Monitoring Toolbox
	- Distributed tracing: in microservice architecture, we give each process a unique ID so that we can associate an error message with a process
	- Stream processing: use Kafka or AWS Kinesis to process events as soon as they are logged to find anomalies. Can also use Flink SQL to find events with specific characteristics. 
	- Dashboards: helpful to visualize but excessive metrics and dashboards can be counterproductive, a phenomenon known as **dashboard rot**. 
	- Alerts: 
		- Alert policy: condition for alert
		- Notification channel: who is to be notified
		- Description of the alert: detailed as possible
- Observability: use outputs of system to bring visibility into behavior of system
	- Observability is about instrumenting your system in a way to ensure that sufficient information about a system’s run time is collected and analyzed: we should be able to figure out what went wrong by looking at the system’s logs and metrics without having to ship new code to the system
	- **Telemetry:** system's outputs collected at runtime. Logs and metrics collected from cloud services or customer devices. 
	- We can't catch all bugs before deployment, but we want to minimize downtime/bad predictions
	- Coarse-grained monitoring: only useful to detect performance issue
		- Full-feedback, partial-feedback, no-feedback ![[Pasted image 20250922155215.png]]
	- IFF the coarse-grained metrics throw an error, we switch to fine-grained metrics
		- Min, max values
		- Unseen categorical values
		- Hypothesis tests on data shifts
		- Missingness

# Continual Learning
* Continually adapt models to changing data distributions: one step past monitoring. Monitoring is *passive*, continual learning is *active*. 
	* Set up system such that models can continuously learn from new data
	* Stateful training
	* Very useful for natural labels (e.g., rec system) or short feedback loops
- How often to update? 
	- Not every sample: leads to catastrophic forgetting and too expensive
	- Update models with micro-batches
	- Important to quantify the value of data freshness: requires offline or online experiments
- Evaluation cadence != Learning cadence
	- Offline evaluation = sanity check
	- Online evaluation: canary analysis, A/B testing
- Canary testing
	- New model alongside existing model
	- Route some traffic to new model
	- Slowly increase traffic to new model
- Interleaved experiments
	- Useful for ranking/recsys 
- Shadow testing
	- New models' predictions are logged but not actioned on 